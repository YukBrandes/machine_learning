# 1 算法简介
决策树是附加概率结果的一个树状的决策图，是直观的运用统计概率分析的图法。机器学习中决策树是一个预测模型，它表示对象属性和对象值之间的一种映射，树中的每一个节点表示对象属性的判断条件，其分支表示符合节点条件的对象。树的叶子节点表示对象所属的预测结果。

决策树算法是最广泛使用的机器学习技术之一，以树形结构建立模型，类似于流程图，数据从根节点开始遍历所有决策点，直至叶子节点。

## 应用领域：
    1.信用评估模型：明确申请拒绝的准则
    2.客户流失或者满意度调查：与管理者和广告公司合作
    3.医疗诊断：基于医疗设备、病人特征数据进行成功性诊断与原因分析
	
## 核心思想
分而治之（递归划分），利用特征的值将数据分解成具有相似类的较小的子集，每次选取最佳候选特征直至达到停止标准。

## 停止标准：
> 1.节点上所有（几乎所有）的案例都属于同一类
> 2.没有剩余的特征辨别案例之间的区别
> 3.决策树已经达到预定义的大小限制

## 主流算法
> 1.CART (Classification And Regression Tree)
> 2.ID3
> 3.C5.0
> 4.随机森林 (Random Forest) 
---

# 2 实验数据
German Credit
---

# 3 评价指标
根据混淆矩阵可以得到评价分类模型的指标有以下几种。
> 分类准确度，就是正负样本分别被正确分类的概率，计算公式为：
> 召回率，就是正样本被识别出的概率，计算公式为：
> 虚警率，就是负样本被错误分为正样本的概率，计算公式为：
> 精确度，就是分类结果为正样本的情况真实性程度，计算公式为：
---

# 4 评估方法
评估方法有保留法、随机二次抽样、交叉验证和自助法等。
> 保留法 (holdout) 是评估分类模型性能的最基本的一种方法。将被标记的原始数据集分成训练集和检验集两份，训练集用于训练分类模型，检验集用于评估分类模型性能。但此方法**不适用样本较小的情况**，模型可能**高度依赖训练集和检验集**的构成。
> 随机二次抽样 (random subsampling) 是指多次重复使用保留方法来改进分类器评估方法。同样此方法也**不适用训练集数量不足**的情况，而且也可能造成有些数据未被用于训练集。
> 交叉验证 (cross-validation) 是指把数据分成数量相同的 k 份，每次使用数据进行分类时，选择其中一份作为检验集，剩下的 k-1 份为训练集，重复 k 次，正好使得每一份数据都被用于一次检验集 k-1 次训练集。该方法的优点是尽可能多的数据作为训练集数据，每一次训练集数据和检验集数据都是相互独立的，并且完全覆盖了整个数据集。也存在一个缺点，就是分类模型运行了 K 次，计算开销较大。
> 自助法 (bootstrap) 是指在其方法中，训练集数据采用的是有放回的抽样，即已经选取为训练集的数据又被放回原来的数据集中，使得该数据有机会能被再一次抽取。**用于样本数不多的情况下**，效果很好。
---

# 5 总结
本次实验通过一个决策树的案例，着重从特征选择、剪枝等方面展示了决策树的R语言实现。